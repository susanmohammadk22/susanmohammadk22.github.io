---
title: "assignment2.qmd"
---

2/6/2026 Question 4: What problems do you encounter when working with the dataset? Answer: 1) when I paste the code inside of the assignment document I received this error. Error in `haven::read_dta()`: ! This kind of input is not handled. Run `rlang::last_trace()` to see where the error occurred. \> rlang::last_trace() <error/rlang_error> Error in `haven::read_dta()`: ! This kind of input is not handled. --- Backtrace: ▆ 1. └─haven::read_dta("https://github.com/datageneration/home/blob/master/DataProgra\nmming/data/TEDS_2016.dta?raw=true") Run rlang::last_trace(drop = FALSE) to see 2 hidden frames.

while Chat GPT said I should write read_stat instead of read_data. 2) I paste this from the book: \> \# Prepare the analyze the Party ID variable \> \# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7="NA") \> \> TEDS_2016$PartyID <- factor(TEDS_2016$PartyID, labels=c("KMT","DPP","NP","PFP", "TSU", "NPP","NA")) \> \# Check the variable \> attach(TEDS_2016) \> head(PartyID) \[1\] NA NA KMT NA NA DPP Levels: KMT DPP NP PFP TSU NPP NA \>

then I received very NA in the first column. Question 5: Answer: then when I installed the \## install.packages("descr"), and run the library I cuold see all data without NA.

TEDS_2016\$PartyID Frequency Percent KMT 388 22.9586 DPP 591 34.9704 NP 3 0.1775 PFP 32 1.8935 TSU 5 0.2959 NPP 43 2.5444 NA 628 37.1598 Total 1690 100.0000 \>

Questuin 8 \> TEDS_2016$Tondu<-as.numeric(TEDS_2016$Tondu,labels=c("Unification now”, + “Status quo, unif. in future”, “Status quo, decide later", "Status quo + forever", "Status quo, indep. in future", "Independence now”, “No response")) \> library(haven) \> \> TEDS_2016$Tondu<-as.numeric(TEDS_2016$Tondu,labels=c("Unification now”, Error: unexpected '\>' in "\>" \> + “Status quo, unif. in future”, “Status quo, decide later", "Status quo Error: unexpected input in "+ “" \> + forever", "Status quo, indep. in future", "Independence now”, “No response")) Error: unexpected string constant in "+ forever", "" \> TEDS_2016$Tondu<-as.numeric(TEDS_2016$Tondu,labels=c("Unification now”, + “Status quo, unif. in future”, “Status quo, decide later", "Status quo + forever", "Status quo, indep. in future", "Independence now”, “No response")) \> TEDS_2016$Tondu<-as.numeric(TEDS_2016$Tondu,labels=c("Unification now”, + “Status quo, unif. in future”, “Status quo, decide later", "Status quo + forever", "Status quo, indep. in future", "Independence now”, “No response")) \> TEDS_2016$Tondu<-as.numeric(TEDS_2016$Tondu,labels=c("Unification now”, “Status quo, unif. in future”, “Status quo, decide later", "Status quo forever", "Status quo, indep. in future", "Independence now”, “No response")) \> attach(TEDS_2016) The following objects are masked from TEDS_2016 (pos = 14):

```         
age, Age, Arear, Blue, Career, Career8, District, DPP, Econ_worse, econworse5, edu, Edu, Ethnic,
female, Govt_dont_care, Govt_for_public, green, Green, highincome, income, income_nm,
Independence, Inequality, inequality5, KMT, lowincome, Mainland_father, Minnan_father, nI2,
No_Party, noparty, north, npp, Party, PartyID, pfp, pubwelf5, Sex, South, sq, Taiwanese, Tondu,
Tondu3, Unification, voteblue, voteblue_nm, votedpp_1, votekmt, votekmt_1, votekmt_nm, votetsai,
votetsai_all, votetsai_nm, whitecollar
```

> head(Tondu) \[1\] 3 5 3 5 9 4 tail(Tondu) \[1\] 2 3 4 4 3 3 library (descr) freq(EDS_2016$Tondu)
> Error: object 'EDS_2016' not found
>  freq(TEDS_2016$Tondu) TEDS_2016\$Tondu Frequency Percent 1 27 1.598 2 180 10.651 3 546 32.308 4 328 19.408 5 380 22.485 6 108 6.391 9 121 7.160 Total 1690 100.000
>
> ggplot(TEDS_2016,aes(Tondu))+ geom_bar() save.image("C:\\Users\\smvic\\Documents\\Knowledge Mining\\second session assignment\\.RData")

I had a challenge when I wanted to make this code \> ggplot(TEDS_2016, aes(x = Tondu)) + + geom_bar(aes(y = (..count..)/sum(..count..))) + + scale_y_continuous(labels = percent) + + xlab("Taiwan–China Status Preference") + + ylab("Percentage of Respondents (%)")

I did not know that what would be the x lable and y lable for this report

2/10/2026

2 questions after class

originality and AI : it means to me that the way we ask questions and ask it prepare or create something that is the original part while the AI by itself cannot create something from the zero as it use algorithm and reading current data and information.

What is AGI? Artifitial General Inteligence this type of AI is smarter and be close to human more compare to others as it can learn new skills. if this becomes reallity will be helpful in terms of finding new remedies for some cancer or finding new medication. but it will be a substitute for human


----------------------------------------------------------------------

Assignment 2- 2/14/25

Q1: Objective: Design prompts to conduct a structured systematic literature review on data
mining and machine learning using multiple AI models (e.g. ChatGPT, Copilot, and Grok 3)
and analyze each model’s contribution and refining prompts for academic precision and
depth.

Step 1: initial prompt creation
Task: Write a baseline prompt to request a structured systematic literature review on data mining and machine learning applications.

**Context**: I am a graduate student working on creating a structured literature review on data mining and machine learning applications 
**Task**: Conduct the structured literature review on data mining and machine learning application between 2012 to 2024 in the USA corporations.

**Constraint**: 1) just 2000 word with Academic word 2) include the strategy, hypothesis, tests and results. 3) Focus on data mining and machine learning 4) start it with recent years researches. 5) for each research write a paragraph including the hypothesis and summary of finding
**Format**: Provide output as paragraphs with the result without bullet point.


Result: I compared DeepSeek, Gemini, and Copilot. the Copilot was the worst as it did not understood what I am talking and start with options when I select what I want again it did not create proper information and it was completely different from what I was asking.Deep Seek was good, it created more content for litreture review. In jeminy it provided short information about researches and it did not show the source of its research while Deepseek provided more information and the link for resources.

Step 2: Analyze Model Responses
Structure: Deepseek provided more detail like the way that I asked it. it mentioned about the theory or hypothesis, It did not identify Gaps or meaningful things but it wrote it like a story from recent studies to later one with mentioning the name of the study and the person and year and the result of that study.while in the Geminy I did not see that much detail about studies and it just mentioned years but not taking about which article or which writer found these results. 


Step 3: Refine the Prompt
o Task: Revise the prompt to address deficiencies in each model’s response,
creating three tailored prompts—one for ChatGPT, one for Copilot, and one
for Grok 3.

Answer: for Copilot I will change it to:
**Context**: I am a graduate student working on creating a structured literature review on data mining and machine learning applications 
**Task**: Conduct a 2000 word academic review on data mining and machine learning application in the USA corporate environement. Provide clear structured methodology section including search title, corporate industry focus, name of the writer and data evaluation metrics. 

**Constraint**: 1) just 2000 word with Academic word 2) include the strategy, hypothesis, tests and results. 3) Focus on data mining and machine learning 4) start it with recent years researches. 5) mention the methodology strenght and weaknesses. , identify Gap and trends and hypothesis
**Format**: Provide output as paragraphs with the result without bullet point.


I would put this for all three.

it now is mentioning all I want this time Copilot had a much better result than its first time and it provided separate paraghraphs for conclusion, methodology ,... so it was more complete even better than Deepseek and Gemini in the structure.but still it does not showing 
the resources that from which link it found these information. but the structure was better
SO I understood that Gemini needs more synthetic structure and more details , deep seek needs more analytical guidance with separete constraints not all constraint in one line and I think Copilot needs both structure and more guidance

Step 4: Cross-Model Collaboration
o Task: Integrate the best elements from each model’s output into a final
systematic review. Write a new prompt for the student’s preferred model (e.g.,Grok 3) to synthesize the results.

Answer: 
**Context**: Produce 2000 word structured systemic litreture review, using the previously generated draft on data mining and machine learning applications in U.S corporations (2012-2024), synthesize the strongest structural organization, methodological clarity, analytically depth, documented trend analysis and hypothesis.
**constraints**:
Ensure the methodology section clearly specifies search strategy, industry scope and metrics for evaluation.
Begin with the most recent studies.
Present synthesized not individually summarized analysis of trends , corporate practices, hypotheses, tests, outcomes, methodology, gaps, weaknesses. 

**Format**:
use formal academic language
show the review result in structured paragraphs without bullet points
show who is the writer name and the title of their study at the beginning of the paragraph

Result:
in Gemini I received a much better result, Copilot could not again provide the title of the resources, Deepseek did not provide a nice structured and it brings some chinees words in the middle of paragraphs but the content was good. the Gemini followed everything I asked and this is the result
this is what I received from Gemini:

This systematic literature review synthesizes the evolution and application of data mining (DM) and machine learning (ML) within United States corporations from 2012 to 2024. The methodology employed for this review utilizes a PRISMA-based search strategy focused on the query "Machine Learning and Data Mining Applications in US Enterprise 2012-2024," targeting high-impact journals in the Scopus and Web of Science databases. The industrial scope encompasses Finance, Healthcare, Retail, and Technology sectors, with data evaluation metrics primarily focused on predictive accuracy (F1-score, AUC-ROC), operational efficiency (ROI), and computational cost (latency/throughput).

The research of Eloundou, Manning, and Mishkin (2024) in their article The Dual Impact of Generative Artificial Intelligence on Corporate Employment and Valuation establishes the current paradigm of "Generative Augmentation" in US tech and service sectors. The authors hypothesize that Generative AI (GenAI) acts as a high-skill labor intensifier rather than a simple replacement tool, particularly for data-intensive roles like accounting and software engineering. Their tests, which aggregate task-based exposure across thousands of firm-level positions, reveal that post-ChatGPT (2022–2024), US firms with higher AI exposure saw increased average wages and initial valuation spikes. The finding indicates that while GenAI provides an average labor cost saving of 25%, the primary hurdle is a "realization gap" where firms struggle to translate potential productivity into tangible quarterly performance, identifying a critical trend toward organizational restructuring as a prerequisite for ML value.

In the healthcare and pharmaceutical sectors, Siva Sai et al. (2024) in The Transformative Impact of Generative AI across Industries: A Comprehensive Systematic Review examine the shift from diagnostic to generative applications. The researchers hypothesized that integrating Large Language Models (LLMs) and Generative Adversarial Networks (GANs) would significantly accelerate drug discovery and medical imaging analysis in the US. By testing real-world scenarios in medical education and clinical diagnostics, the study found that GAN-based data mining could generate high-fidelity synthetic patient data, overcoming the privacy barriers of the HIPAA era. The outcome demonstrates that while diagnostic accuracy improved, the methodology’s weakness lies in "hallucination" risks and ethical biases, highlighting a research gap in regulatory frameworks for autonomous medical AI within US corporations.

Shifting to the 2020–2022 "Big Data Velocity" era, Uddin et al. (2023) in their review Recent Trends in Machine Learning for Healthcare Big Data Applications focus on the scalability of tree-based models and AutoML. Their central hypothesis suggested that feature set selection (FSS) is more critical for corporate efficiency than the choice of the neural network itself. By testing the Tree-based Pipeline Optimization Tool (TPOT) on large-scale RNA-Seq datasets, they found that TPOT-FSS completed tasks 17 times faster than standard models without sacrificing accuracy. This results in a summary of findings that emphasizes the "Green AI" trend, where US corporations are moving away from massive compute-heavy models toward computationally efficient, power-aware ML pipelines to meet sustainability goals.

Within the financial sector, Alshater et al. (2022) in Artificial Intelligence in Financial Market Prediction analyze the mid-period (2016–2019) transition toward deep reinforcement learning for stock market forecasting. The article hypothesizes that combining traditional data mining of historical prices with real-time sentiment analysis from social media would yield a statistically significant alpha in high-frequency trading. The tests, utilizing Random Forests and Gradient Boosting Machines, confirmed that multimodal data sources enhanced prediction accuracy by providing a "holistic perspective" on market sentiment. However, the study identifies a methodological weakness in "model drift," where ML models trained on pre-pandemic data failed to generalize during the 2020 volatility, revealing a gap in the literature regarding "Robustness under Crisis" for corporate financial algorithms.

The foundational period of 2012–2015 is characterized by Chaudhary et al. (2016) in their synthesis of "Big Data Mining" in US retail. The hypothesis of this era was that moving from descriptive Business Intelligence (BI) to predictive data mining could reduce inventory overhead by 15% through more accurate demand forecasting. Tests performed on US retail giants using Hadoop and Spark frameworks showed that early ML models (SVM and Logistic Regression) were highly effective at detecting fraud and predicting churn, but were limited by the "siloed" nature of corporate data. This results in a summary of findings that points to the trend of "Data Democratization," where the methodology's strength was its ability to handle volume, but its weakness was a lack of real-time adaptability, a gap that was eventually filled by the streaming analytics of the late 2010s.

The synthesized trend analysis across the 2012–2024 spectrum reveals a clear trajectory from retrospective data mining to autonomous antigenic ML. The primary research gap identified is the "governance-efficiency paradox," where the technical capacity for corporate automation outpaces the legal and ethical frameworks required to manage it. While current US corporate practices are moving toward "Small Language Models" (SLMs) and decentralized "Federated Learning" to protect data privacy, the literature shows a persistent weakness in the lack of longitudinal studies on AI's impact on corporate culture and long-term skill degradation. The review concludes that the next frontier of US corporate ML will focus on "Explainability and Trust," moving the hypothesis from purely performance-based metrics to "Accountable Intelligence."
this is the result of what I received from Gemini. not my work it is write by AI- Gemini

Step 5: Reflection
Task: Write a reflection answering:
1) How did each model approach the systematic review differently? in copilot it produce a segmented and academically format for the review but it was not reliable as it did not mentioned about the resources. in Gemini it shows methodology and spoke about the study, its metrics and outcome. deppseek created more content than other AI models and offered more details of each study also mentioned the resources. so I would rely on deep seek as it provide more detail information about studies.

2) Which prompt refinements yielded the best results for each model? for copilot the structure was the best result that I wanted of a formal literature review result with academic word and format but without resources. Gemini needed methodology details and metrics. Deep seek was good in terms of the details and resources. 

3) I learned that AI models need a precies prompt and structured one of what we want from them to generate the appropriate result for us. so it is a best collaborative option but in the end we need to check the results and update it in a way that we want. 

3) What did you learn about leveraging AI for structured academic
reviews?



