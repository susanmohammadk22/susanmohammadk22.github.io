---
title: "assignment1.qmd"
---
To Explain or to Predict?
They talked about the relationship between causal explanation and empirical prediction.
The some fields causal explanatory model has a high prediction power. While in empirical model like NLP the focus is on empirical prediction.
The main hypothesis in this study is causal versus predictive distinction has high effect on statistical modeling process and its results. 
Explanatory modeling and predictive modeling reflect the data mining and statistical analysis methos to explain and predict. And modeling means the whole process including definition, study design, and data collection for statistical usage.
In this study they define explaining as causal explanation and explanatory needling as the use of statistical models for testing causal explanations.
Predictive modeling uses data mining algorithms and statistical models to predict future observations until time t+k. these modeling summarized the data structure. While in causal theory (descriptive modeling) rely on measurable model to describe the data, like a regression model is used to capture dependent and independent variables to show the relationships between variables. They consider four area for differences between explanation and prediction models, including : 
Causation-association: in explanatory model f represents causal relationship, while in predictive model f records correlation between inputs and outputs.
Theory data: in explanatory, f model is created based on the theory to be able to explain while in predictive, the explanatory and clarity of model is in lower priority than data.
Retrospective-prospective: in explanatory relies on testing the hypothesis of previous or current observation, while in prediction assumes the future observations.
Bias-variance: in explanatory modeling we attempt on minimizing the bias to obtain more accurate representation. While in predictive models the try to maximize the combination of bias and estimation variance to reach improved empirical precision.
These are the steps in the statistical modeling process:
Define goal, design study and collect data, prepare data, eda, choose variables, choose methods, evaluate, validate, and model selection, use mode and report

They said consider both explanation power and prediction power as two dimension, as they are different qualities.  There are some explanatory model have some level of predictive power to be considered scientifically useful and there are some predictive models have sufficient explanatory power to be scientifically useful. While , there exist predictive models that do not properly “explain” yet are scientifically valuable. (Bill Langford, Woit, 2006, pages x xii). Therefore, we should define our goal to optimize the right criteria and report both qualities to provide a complete picture of the model.
Finally as statistics focuses on inference and less focus on predictive analysis, machine learning has been created. 


Statistical modeling 
Two cultures
One culture is data are generated by a given data model
Second culture use algorithms and treats the data mechanism as unknown
As statistical community relies on data models, it kept statisticians from working a large range of interesting problems.
In the article Leo Brieman defines two goal in analyzing data. 1)prediction: predict future responses to future input variables 2) information: extract information about how nature connects response variables to input variables.
He defines two different approaches toward the goals
Data modeling cuture. It assumes a stocustic data model for inside of the black box. The value of parameters are estimated from data and model
The algorithmic modeling culture. The inside of the box is unknown and an algorithmic operatrion can accurately predict.
After several projects and statistical research he found that the belief in the infallibility of data models was almost religious. It is a strange phenomenon- once model created, it becomes truth and the conclusion from it are infallible. While it is not correct. When we make small changes in the model can change complete while it is accurate.
 He said the picture of which covariates are important can vary significantly between two models having about same deviance. Aggregate over a large set of competing models can reduce the non-uniqueness while improving accuracy
He believes that statisticians should focus on solving the problem instead of asking what data model they can create. The best solution might  ba algorithmic model or data model or combination of both. So they need to use wide variety of tools. The roots of statistics is lie on working with data and checking theory against data. And he hoped that this century our field will return t its toots.

